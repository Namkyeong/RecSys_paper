{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data.train\n",
    "test = data.test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatrixFactorization():\n",
    "    \n",
    "    def __init__(self, train, test, k, learning_rate, epochs, verbose = False):\n",
    "        \"\"\"\n",
    "        param R : Rating Matrix\n",
    "        param k : latent parameter\n",
    "        param learning_rate : alpha on weight update\n",
    "        param epochs : training epochs\n",
    "        param verbose : print status\n",
    "        \"\"\"\n",
    "        \n",
    "        self._R = train\n",
    "        self._test = test\n",
    "        self._I = np.array(np.vectorize(lambda x: 0 if x==0 else 1)(train), dtype = np.float64) # indicator matrix\n",
    "        self._n_user_rated = np.sum(self._I, axis = 1)\n",
    "        self._n_item_rated = np.sum(self._I, axis = 0)\n",
    "        self._num_users, self._num_items = train.shape\n",
    "        # sigma?\n",
    "        self._lambda = 0.01\n",
    "        self._k = k\n",
    "        self._learning_rate = learning_rate\n",
    "        self._epochs = epochs\n",
    "        self._verbose = verbose\n",
    "        \n",
    "        \n",
    "    def fit(self):\n",
    "        \"\"\"\n",
    "        training Matrix Factorization : update matrix latent weight and bias\n",
    "        \"\"\"\n",
    "        # init latent features\n",
    "        self._Y = np.random.normal(0, 0.1, size=(self._num_users, self._k))\n",
    "        self._V = np.random.normal(0, 0.1, size=(self._num_items, self._k))\n",
    "        self._W = np.random.normal(0, 0.1, size=(self._num_items, self._k))\n",
    "        \n",
    "        self._training_process = []\n",
    "        for epoch in range(self._epochs):\n",
    "            for i in range(self._num_users):\n",
    "                for j in range(self._num_items):\n",
    "                    if self._R[i, j] > 0 :\n",
    "                        self.gradient_descent(i, j, self._R[i, j])\n",
    "                        \n",
    "            train_cost, test_cost = self.cost()\n",
    "            self._training_process.append((epoch, train_cost, test_cost))\n",
    "            \n",
    "            rank = self.compute_rank()\n",
    "            print(\"Iteration : %d, train_cost = %.4f, test_cost = %.4f, rank = %.4f\" % (epoch+1, train_cost, test_cost, rank))\n",
    "                \n",
    "    \n",
    "    def cost(self):\n",
    "        \"\"\"\n",
    "        compute RMSE\n",
    "        \"\"\"\n",
    "        xi, yi = self._R.nonzero() # 0 이 아닌 값의 index 반환\n",
    "        test_x, test_y = self._test.nonzero()\n",
    "        predicted = self.get_complete_matrix()\n",
    "        cost_train = 0\n",
    "        cost_test = 0\n",
    "        \n",
    "        for x, y in zip(xi, yi):\n",
    "            cost_train += pow(self._R[x, y] - predicted[x, y], 2)\n",
    "        \n",
    "        for x, y in zip(test_x, test_y):\n",
    "            cost_test += pow(self._test[x, y] - predicted[x, y], 2)\n",
    "        \n",
    "        return np.sqrt(cost_train/len(xi)), np.sqrt(cost_test/len(test_x))\n",
    "    \n",
    "    \n",
    "    def gradient_descent(self, i, j, rating):\n",
    "        \"\"\"\n",
    "        gradient descent function\n",
    "        param i : user index\n",
    "        param j : item index\n",
    "        param rating : rating of (i, j)\n",
    "        \"\"\"\n",
    "        prediction = self.get_prediction(i, j)\n",
    "        error = rating - prediction\n",
    "        \n",
    "        # self._U[i, :] += self._learning_rate * ( error * self._V[j, :] - self._lambda_U / self._n_user_rated[i] * self._U[i, :])\n",
    "        # self._V[j, :] += self._learning_rate * ( error * self._U[i, :] - self._lambda_V / self._n_item_rated[j] * self._V[j, :])\n",
    "        \n",
    "        self._Y[i, :] += self._learning_rate * (error * self._V[j, :] - self._lambda * self._Y[i, :])\n",
    "        self._V[j, :] += self._learning_rate * (error * (self._Y[i, :] + self._I[i, :].dot(self._W)/self._n_user_rated[i]) - self._lambda * self._V[j, :])\n",
    "        self._W += self._learning_rate * (error * np.outer(self._I[i, :], self._V[j, :])/self._n_user_rated[i])\n",
    "        self._W[j, :] -= self._learning_rate * self._lambda * self._W[j, :]\n",
    "        \n",
    "    \n",
    "    def get_prediction(self, i, j):\n",
    "        \"\"\"\n",
    "        get predicted rating by user i on item j\n",
    "        param i : user index\n",
    "        param j : item index\n",
    "        \"\"\"\n",
    "        \n",
    "        return np.dot((self._Y[i, :] + self._I[i, :].dot(self._W)/self._n_user_rated[i]), self._V[j, :])\n",
    "    \n",
    "    \n",
    "    def get_complete_matrix(self):\n",
    "        \"\"\"\n",
    "        compute complete matrix\n",
    "        \"\"\"\n",
    "        \n",
    "        return np.dot((self._Y + self._I.dot(self._W) / self._n_user_rated[:, np.newaxis]), self._V.T)\n",
    "    \n",
    "    \n",
    "    def compute_rank(self):\n",
    "        \n",
    "        prediction = self.get_complete_matrix()\n",
    "        test_x = np.unique(self._test.nonzero()[0])\n",
    "        temp_1 = 0\n",
    "        temp_2 = 0\n",
    "        \n",
    "        for x in test_x :\n",
    "            temp_y = self._test[x].nonzero()\n",
    "            inv_pre = -1 * prediction[x, temp_y]\n",
    "            sort_x = inv_pre.argsort() # index starts with 0\n",
    "            sort_x = sort_x.argsort()\n",
    "            rank_x = sort_x / len(sort_x[0])\n",
    "            \n",
    "            temp_1 += (self._test[x, temp_y] * rank_x).sum()\n",
    "            temp_2 += self._test[x, temp_y].sum()\n",
    "        \n",
    "        rank = temp_1 / temp_2\n",
    "            \n",
    "        return rank\n",
    "    \n",
    "    \n",
    "    def print_results(self):\n",
    "        \"\"\"\n",
    "        print fit results\n",
    "        \"\"\"\n",
    "\n",
    "        print(\"Final R matrix:\")\n",
    "        print(self.get_complete_matrix())\n",
    "        print(\"Final RMSE:\")\n",
    "        print(self._training_process[self._epochs-1][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration : 1, train_cost = 3.6970, test_cost = 3.7199, rank = 0.4885\n",
      "Iteration : 2, train_cost = 3.6870, test_cost = 3.7146, rank = 0.4874\n",
      "Iteration : 3, train_cost = 3.6522, test_cost = 3.6911, rank = 0.4828\n",
      "Iteration : 4, train_cost = 3.5204, test_cost = 3.5944, rank = 0.4725\n",
      "Iteration : 5, train_cost = 3.1124, test_cost = 3.2750, rank = 0.4665\n",
      "Iteration : 6, train_cost = 2.4254, test_cost = 2.6604, rank = 0.4650\n",
      "Iteration : 7, train_cost = 1.8994, test_cost = 2.0955, rank = 0.4639\n",
      "Iteration : 8, train_cost = 1.6035, test_cost = 1.7526, rank = 0.4624\n",
      "Iteration : 9, train_cost = 1.4239, test_cost = 1.5473, rank = 0.4607\n",
      "Iteration : 10, train_cost = 1.3066, test_cost = 1.4151, rank = 0.4592\n",
      "Iteration : 11, train_cost = 1.2258, test_cost = 1.3246, rank = 0.4580\n",
      "Iteration : 12, train_cost = 1.1677, test_cost = 1.2598, rank = 0.4571\n",
      "Iteration : 13, train_cost = 1.1244, test_cost = 1.2117, rank = 0.4562\n",
      "Iteration : 14, train_cost = 1.0911, test_cost = 1.1749, rank = 0.4556\n",
      "Iteration : 15, train_cost = 1.0648, test_cost = 1.1462, rank = 0.4550\n",
      "Iteration : 16, train_cost = 1.0437, test_cost = 1.1233, rank = 0.4546\n",
      "Iteration : 17, train_cost = 1.0263, test_cost = 1.1048, rank = 0.4542\n",
      "Iteration : 18, train_cost = 1.0118, test_cost = 1.0894, rank = 0.4539\n",
      "Iteration : 19, train_cost = 0.9995, test_cost = 1.0767, rank = 0.4537\n",
      "Iteration : 20, train_cost = 0.9889, test_cost = 1.0658, rank = 0.4535\n",
      "Iteration : 21, train_cost = 0.9797, test_cost = 1.0566, rank = 0.4533\n",
      "Iteration : 22, train_cost = 0.9716, test_cost = 1.0486, rank = 0.4532\n",
      "Iteration : 23, train_cost = 0.9644, test_cost = 1.0417, rank = 0.4531\n",
      "Iteration : 24, train_cost = 0.9579, test_cost = 1.0356, rank = 0.4530\n",
      "Iteration : 25, train_cost = 0.9521, test_cost = 1.0302, rank = 0.4529\n",
      "Iteration : 26, train_cost = 0.9467, test_cost = 1.0255, rank = 0.4528\n",
      "Iteration : 27, train_cost = 0.9418, test_cost = 1.0212, rank = 0.4527\n",
      "Iteration : 28, train_cost = 0.9373, test_cost = 1.0173, rank = 0.4527\n",
      "Iteration : 29, train_cost = 0.9331, test_cost = 1.0139, rank = 0.4526\n",
      "Iteration : 30, train_cost = 0.9292, test_cost = 1.0107, rank = 0.4526\n",
      "Iteration : 31, train_cost = 0.9255, test_cost = 1.0078, rank = 0.4525\n",
      "Iteration : 32, train_cost = 0.9220, test_cost = 1.0052, rank = 0.4525\n",
      "Iteration : 33, train_cost = 0.9187, test_cost = 1.0027, rank = 0.4525\n",
      "Iteration : 34, train_cost = 0.9155, test_cost = 1.0005, rank = 0.4524\n",
      "Iteration : 35, train_cost = 0.9125, test_cost = 0.9984, rank = 0.4524\n",
      "Iteration : 36, train_cost = 0.9095, test_cost = 0.9964, rank = 0.4523\n",
      "Iteration : 37, train_cost = 0.9067, test_cost = 0.9946, rank = 0.4523\n",
      "Iteration : 38, train_cost = 0.9040, test_cost = 0.9929, rank = 0.4522\n",
      "Iteration : 39, train_cost = 0.9013, test_cost = 0.9913, rank = 0.4522\n",
      "Iteration : 40, train_cost = 0.8987, test_cost = 0.9898, rank = 0.4521\n",
      "Iteration : 41, train_cost = 0.8961, test_cost = 0.9884, rank = 0.4521\n",
      "Iteration : 42, train_cost = 0.8936, test_cost = 0.9870, rank = 0.4521\n",
      "Iteration : 43, train_cost = 0.8912, test_cost = 0.9858, rank = 0.4521\n",
      "Iteration : 44, train_cost = 0.8887, test_cost = 0.9845, rank = 0.4520\n",
      "Iteration : 45, train_cost = 0.8863, test_cost = 0.9834, rank = 0.4520\n",
      "Iteration : 46, train_cost = 0.8839, test_cost = 0.9823, rank = 0.4519\n",
      "Iteration : 47, train_cost = 0.8816, test_cost = 0.9812, rank = 0.4519\n",
      "Iteration : 48, train_cost = 0.8792, test_cost = 0.9802, rank = 0.4518\n",
      "Iteration : 49, train_cost = 0.8769, test_cost = 0.9792, rank = 0.4518\n",
      "Iteration : 50, train_cost = 0.8745, test_cost = 0.9782, rank = 0.4518\n",
      "Iteration : 51, train_cost = 0.8722, test_cost = 0.9773, rank = 0.4517\n",
      "Iteration : 52, train_cost = 0.8698, test_cost = 0.9764, rank = 0.4517\n",
      "Iteration : 53, train_cost = 0.8675, test_cost = 0.9755, rank = 0.4517\n",
      "Iteration : 54, train_cost = 0.8652, test_cost = 0.9747, rank = 0.4516\n",
      "Iteration : 55, train_cost = 0.8628, test_cost = 0.9739, rank = 0.4516\n",
      "Iteration : 56, train_cost = 0.8605, test_cost = 0.9731, rank = 0.4516\n",
      "Iteration : 57, train_cost = 0.8581, test_cost = 0.9723, rank = 0.4515\n",
      "Iteration : 58, train_cost = 0.8558, test_cost = 0.9716, rank = 0.4515\n",
      "Iteration : 59, train_cost = 0.8534, test_cost = 0.9709, rank = 0.4515\n",
      "Iteration : 60, train_cost = 0.8510, test_cost = 0.9702, rank = 0.4514\n",
      "Iteration : 61, train_cost = 0.8486, test_cost = 0.9695, rank = 0.4513\n",
      "Iteration : 62, train_cost = 0.8462, test_cost = 0.9688, rank = 0.4513\n",
      "Iteration : 63, train_cost = 0.8438, test_cost = 0.9681, rank = 0.4513\n",
      "Iteration : 64, train_cost = 0.8414, test_cost = 0.9675, rank = 0.4512\n",
      "Iteration : 65, train_cost = 0.8390, test_cost = 0.9669, rank = 0.4512\n",
      "Iteration : 66, train_cost = 0.8366, test_cost = 0.9663, rank = 0.4511\n",
      "Iteration : 67, train_cost = 0.8342, test_cost = 0.9657, rank = 0.4511\n",
      "Iteration : 68, train_cost = 0.8317, test_cost = 0.9651, rank = 0.4511\n",
      "Iteration : 69, train_cost = 0.8293, test_cost = 0.9646, rank = 0.4510\n",
      "Iteration : 70, train_cost = 0.8268, test_cost = 0.9640, rank = 0.4510\n",
      "Iteration : 71, train_cost = 0.8243, test_cost = 0.9635, rank = 0.4509\n",
      "Iteration : 72, train_cost = 0.8219, test_cost = 0.9630, rank = 0.4509\n",
      "Iteration : 73, train_cost = 0.8194, test_cost = 0.9625, rank = 0.4509\n",
      "Iteration : 74, train_cost = 0.8169, test_cost = 0.9620, rank = 0.4509\n",
      "Iteration : 75, train_cost = 0.8144, test_cost = 0.9616, rank = 0.4508\n",
      "Iteration : 76, train_cost = 0.8119, test_cost = 0.9611, rank = 0.4508\n",
      "Iteration : 77, train_cost = 0.8094, test_cost = 0.9607, rank = 0.4508\n",
      "Iteration : 78, train_cost = 0.8069, test_cost = 0.9603, rank = 0.4508\n",
      "Iteration : 79, train_cost = 0.8044, test_cost = 0.9599, rank = 0.4507\n",
      "Iteration : 80, train_cost = 0.8019, test_cost = 0.9595, rank = 0.4507\n",
      "Iteration : 81, train_cost = 0.7994, test_cost = 0.9592, rank = 0.4507\n",
      "Iteration : 82, train_cost = 0.7969, test_cost = 0.9588, rank = 0.4506\n",
      "Iteration : 83, train_cost = 0.7944, test_cost = 0.9585, rank = 0.4506\n",
      "Iteration : 84, train_cost = 0.7919, test_cost = 0.9582, rank = 0.4506\n",
      "Iteration : 85, train_cost = 0.7894, test_cost = 0.9579, rank = 0.4506\n",
      "Iteration : 86, train_cost = 0.7869, test_cost = 0.9577, rank = 0.4506\n",
      "Iteration : 87, train_cost = 0.7843, test_cost = 0.9574, rank = 0.4505\n",
      "Iteration : 88, train_cost = 0.7818, test_cost = 0.9572, rank = 0.4505\n",
      "Iteration : 89, train_cost = 0.7793, test_cost = 0.9569, rank = 0.4505\n",
      "Iteration : 90, train_cost = 0.7768, test_cost = 0.9567, rank = 0.4505\n",
      "Iteration : 91, train_cost = 0.7743, test_cost = 0.9565, rank = 0.4505\n",
      "Iteration : 92, train_cost = 0.7718, test_cost = 0.9564, rank = 0.4504\n",
      "Iteration : 93, train_cost = 0.7692, test_cost = 0.9562, rank = 0.4504\n",
      "Iteration : 94, train_cost = 0.7667, test_cost = 0.9560, rank = 0.4504\n",
      "Iteration : 95, train_cost = 0.7642, test_cost = 0.9559, rank = 0.4504\n",
      "Iteration : 96, train_cost = 0.7617, test_cost = 0.9558, rank = 0.4504\n",
      "Iteration : 97, train_cost = 0.7592, test_cost = 0.9557, rank = 0.4504\n",
      "Iteration : 98, train_cost = 0.7567, test_cost = 0.9556, rank = 0.4504\n",
      "Iteration : 99, train_cost = 0.7542, test_cost = 0.9555, rank = 0.4504\n",
      "Iteration : 100, train_cost = 0.7517, test_cost = 0.9555, rank = 0.4504\n",
      "Final R matrix:\n",
      "[[3.5660669  3.34076066 3.32368715 ... 0.50739024 1.12679457 1.41423354]\n",
      " [3.95305379 3.16214768 2.95584289 ... 0.7108749  1.00532895 1.21644009]\n",
      " [3.26465192 2.63730949 2.53635481 ... 0.59459288 0.70771776 1.02030751]\n",
      " ...\n",
      " [4.39816153 3.47385561 3.1971421  ... 0.80690573 0.98100972 1.29755148]\n",
      " [4.54411234 3.72469593 3.18374936 ... 0.67751735 1.09606217 1.14167587]\n",
      " [3.48622449 3.64545948 2.88218844 ... 0.67017191 0.83828519 1.22562019]]\n",
      "Final RMSE:\n",
      "0.955472117246004\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(7)\n",
    "    \n",
    "np.seterr(all=\"warn\")\n",
    "    \n",
    "factorizer = MatrixFactorization(train, test, k=40, learning_rate=0.001, epochs=100, verbose=True)\n",
    "# regression parameter 2개\n",
    "factorizer.fit()\n",
    "factorizer.print_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mf_v1",
   "language": "python",
   "name": "mf_v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
