{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## implementation of Bayesian Personalized Ranking for Implicit Feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data.train\n",
    "test = data.test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatrixFactorization():\n",
    "    \n",
    "    def __init__(self, train, test, k, learning_rate, reg_param, epsilon):\n",
    "        \"\"\"\n",
    "        param R : Rating Matrix\n",
    "        param k : latent parameter\n",
    "        param learning_rate : alpha on weight update\n",
    "        param reg_param : regularization parameter\n",
    "        \"\"\"\n",
    "        \n",
    "        self._R = train\n",
    "        self._test = test\n",
    "        self._X = np.array(np.vectorize(lambda x: 0 if x==0 else 1)(train), dtype = np.float64) # create X matrix : implicit feedbacks (binary)\n",
    "        self._num_users, self._num_items = train.shape\n",
    "        self._k = k\n",
    "        self._learning_rate = learning_rate\n",
    "        self._reg_param = reg_param\n",
    "        self._epsilon = epsilon\n",
    "        \n",
    "        \n",
    "    def fit(self):\n",
    "        \"\"\"\n",
    "        training Matrix Factorization : update matrix latent weight and bias\n",
    "        \"\"\"\n",
    "        \n",
    "        # init latent features\n",
    "        self._W = np.random.normal(scale = 1.0/self._k, size=(self._num_users, self._k))\n",
    "        self._H = np.random.normal(scale = 1.0/self._k, size=(self._num_items, self._k))\n",
    "        \n",
    "        \n",
    "        # train until cost converges\n",
    "        self._training_process = []\n",
    "        while converge == True :\n",
    "            \n",
    "            count += 1\n",
    "                        \n",
    "            train_cost, test_cost = self.cost()\n",
    "            self._training_process.append((count, train_cost, test_cost))\n",
    "            \n",
    "            print(\"Iteration : %d, train_cost = %.4f, test_cost = %.4f\" % (epoch+1, train_cost, test_cost))\n",
    "    \n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        \"\"\"\n",
    "        return sigmoid \n",
    "        \"\"\"\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    \n",
    "    def log_posterior(self):\n",
    "        \"\"\"\n",
    "        compute log posterior of params\n",
    "        \"\"\"\n",
    "        lp = 0\n",
    "        non_zero_u = self._R.nonzero()[0] # 0 이 아닌 값의 index 반환\n",
    "        \n",
    "        for u in non_zero_u :\n",
    "            temp_i = self._R[u].nonzero()\n",
    "            for i in temp_i:\n",
    "                for j in temp_i:\n",
    "                    lp += np.log(self.sigmoid(self._W[u].dot((self._H[i]-self._H[j]).T)))\n",
    "                    \n",
    "        return lp - self._reg_param * (np.linalg.norm(self._W)+np.linalg.norm(self._H))\n",
    "    \n",
    "    \n",
    "    def gradient_descent(self, u, i, j):\n",
    "        \"\"\"\n",
    "        gradient descent function\n",
    "        param u : user index\n",
    "        param i : item index\n",
    "        param j :\n",
    "        param rating : rating of (u, i)\n",
    "        \"\"\"\n",
    "        \n",
    "        prediction = self.get_prediction(u,i)\n",
    "        error = rating - prediction\n",
    "        \n",
    "        self._b_P[u] += self._learning_rate * (error - self._reg_param * self._b_P[u])\n",
    "        self._b_Q[i] += self._learning_rate * (error - self._reg_param * self._b_Q[i])\n",
    "        \n",
    "        dp, dq, dy, j = self.gradient(error, u, i)\n",
    "        self._P[u, :] += self._learning_rate * dp\n",
    "        self._Q[i, :] += self._learning_rate * dq\n",
    "        self._Y[j, :] += self._learning_rate * dy\n",
    "        \n",
    "        \n",
    "    \n",
    "    def get_prediction(self, u, i):\n",
    "        \"\"\"\n",
    "        get predicted rating by user i on item j\n",
    "        \"\"\"\n",
    "        \n",
    "        return self._b + self._b_P[u] + self._b_Q[i] + self._Q[i, :].T.dot(self._P[u, :] + self._N[u] * self._Y[self._mask[u,:], :].sum(axis=0))\n",
    "\n",
    "    \n",
    "    def get_complete_matrix(self):\n",
    "        \"\"\"\n",
    "        compute complete matrix\n",
    "        \"\"\"\n",
    "        \n",
    "        predictions = np.zeros([self._num_users, self._num_items])\n",
    "        for u in range(self._num_users):\n",
    "            for i in range(self._num_items):\n",
    "                predictions[u, i] = self.get_prediction(u, i)\n",
    "                \n",
    "        predictions = np.array(predictions, dtype = np.float64)\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    \n",
    "    def print_results(self):\n",
    "        \"\"\"\n",
    "        print fit results\n",
    "        \"\"\"\n",
    "\n",
    "        print(\"Final R matrix:\")\n",
    "        print(self.get_complete_matrix())\n",
    "        print(\"Final RMSE:\")\n",
    "        print(self._training_process[self._epochs-1][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration : 10, train_cost = 0.9487, test_cost = 0.9886\n",
      "Iteration : 20, train_cost = 0.9196, test_cost = 0.9632\n",
      "Iteration : 30, train_cost = 0.9069, test_cost = 0.9540\n",
      "Iteration : 40, train_cost = 0.8935, test_cost = 0.9459\n",
      "Iteration : 50, train_cost = 0.8784, test_cost = 0.9377\n",
      "Iteration : 60, train_cost = 0.8636, test_cost = 0.9310\n",
      "Iteration : 70, train_cost = 0.8480, test_cost = 0.9255\n",
      "Iteration : 80, train_cost = 0.8311, test_cost = 0.9212\n",
      "Iteration : 90, train_cost = 0.8126, test_cost = 0.9177\n",
      "Iteration : 100, train_cost = 0.7928, test_cost = 0.9152\n",
      "Final R matrix:\n",
      "[[3.92795972 3.15884938 3.00837489 ... 3.10269225 3.50778327 3.33668068]\n",
      " [3.72351488 3.18190694 2.97444506 ... 3.32456955 3.63207592 3.54908656]\n",
      " [3.38509121 2.68161406 2.76964097 ... 2.9511531  3.19912569 3.16336248]\n",
      " ...\n",
      " [4.2410321  3.58504866 3.38590671 ... 3.57237379 3.79353462 3.73546977]\n",
      " [4.55871044 3.9119182  3.42007767 ... 3.69270164 3.94635209 3.84667653]\n",
      " [3.91530559 3.31412494 3.22606723 ... 3.05291738 3.29505982 3.22377742]]\n",
      "Final RMSE:\n",
      "0.9151653594939605\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(7)\n",
    "np.seterr(all=\"warn\")\n",
    "    \n",
    "factorizer = MatrixFactorization(train, test, k=40, learning_rate=0.001, reg_param=0.001, epochs=100, verbose=True)\n",
    "\n",
    "# regression parameter 2개\n",
    "factorizer.fit()\n",
    "factorizer.print_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mf_v1",
   "language": "python",
   "name": "mf_v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
