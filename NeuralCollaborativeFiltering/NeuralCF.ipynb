{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of Neural Collaborative Filtering\n",
    "https://arxiv.org/abs/1708.05031"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataframe\n",
    "import data\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select GPU Number\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if cuda available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "torch.manual_seed(315)\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.manual_seed_all(912)\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalized Matrix Factorization\n",
    "MF can be interpreted as a special case of our NCF framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GMF(nn.Module):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super(GMF, self).__init__() # run nn.Module.__init__()\n",
    "        self.num_users = config[\"num_users\"]\n",
    "        self.num_items = config[\"num_items\"]\n",
    "        self.f = config[\"latent_dim\"]\n",
    "        \n",
    "        # Embedding Layer\n",
    "        self.embedding_user = nn.Embedding(num_embeddings = self.num_users, embedding_dim = self.f)\n",
    "        self.embedding_item = nn.Embedding(num_embeddings = self.num_items, embedding_dim = self.f)\n",
    "        \n",
    "        # One layer\n",
    "        self.affine_output = nn.Linear(in_features = self.f, out_features = 1)\n",
    "        self.logistic = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, u, i):\n",
    "        \n",
    "        user_embedding = self.embedding_user(u)\n",
    "        item_embedding = self.embedding_item(i)\n",
    "        product = torch.mul(user_embedding, item_embedding) # element-wise product\n",
    "        logits = self.affine_output(product)\n",
    "        rating = self.logistic(logits)\n",
    "        \n",
    "        return rating\n",
    "    \n",
    "    def init_weight(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super(MLP, self).__init__()\n",
    "        self.num_users = config[\"num_users\"]\n",
    "        self.num_items = config[\"num_items\"]\n",
    "        self.f = config[\"latent_dim\"]\n",
    "        \n",
    "        # Build Layers\n",
    "        ## Embedding Layer\n",
    "        self.embedding_user = nn.Embedding(num_embeddings = self.num_users, embedding_dim = self.f)\n",
    "        self.embedding_item = nn.Embedding(num_embeddings = self.num_items, embedding_dim = self.f)\n",
    "        \n",
    "        ## Fully Connected Layer\n",
    "        self.fc_layers = nn.ModuleList() # holds submodules in a list\n",
    "        for idx, (in_size, out_size) in enumerate(zip(config[\"layers\"][:-1], config[\"layers\"][1:])):\n",
    "            self.fc_layers.append(nn.Linear(in_size, out_size))\n",
    "        \n",
    "        ## Final Layer\n",
    "        self.affine_output = nn.Linear(in_features = config[\"layers\"][-1], out_features = 1)\n",
    "        self.logistic = nn.Sigmoid()\n",
    "        \n",
    "    \n",
    "    def forward(self, u, i):\n",
    "        \n",
    "        user_embedding = self.embedding_user(u)\n",
    "        item_embedding = self.embedding_item(i)\n",
    "        vector = torch.cat([user_embedding, item_embedding], dim=-1) # concatenate user, item\n",
    "        for idx, _ in enumerate(range(len(self.fc_layers))):\n",
    "            vector = self.fc_layers[idx](vector)\n",
    "            vector = nn.ReLU()(vector)\n",
    "        logits = self.affine_output(vector)\n",
    "        rating = self.logistic(logits)\n",
    "        \n",
    "        return rating\n",
    "    \n",
    "    \n",
    "    def init_weight(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Matrix Factorization\n",
    "We allow GMF and MLP to learn separate embeddings, and combine the two models by concatenating their last hidden layer  \n",
    "This model combines the linearity of MF and non-linearity of Deep Neural Networks for modelling user-item latent structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuMF(nn.Module):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super(NeuMF, self).__init__()\n",
    "        self.config = config\n",
    "        self.num_users = config[\"num_users\"]\n",
    "        self.num_items = config[\"num_items\"]\n",
    "        self.f_MF = config[\"latent_dim_MF\"]\n",
    "        self.f_MLP = config[\"latent_dim_MLP\"]\n",
    "        \n",
    "        self.embedding_user_MF = nn.Embedding(num_embeddings = self.num_users, embedding_dim = self.f_MF)\n",
    "        self.embedding_item_MF = nn.Embedding(num_embeddings = self.num_items, embedding_dim = self.f_MF)\n",
    "        self.embedding_user_MLP = nn.Embedding(num_embeddings = self.num_users, embedding_dim = self.f_MLP)\n",
    "        self.embedding_item_MLP = nn.Embedding(num_embeddings = self.num_items, embedding_dim = self.f_MLP)\n",
    "        \n",
    "        self.fc_layers = nn.ModuleList()\n",
    "        for idx, (in_size, out_size) in enumerate(zip(config[\"layers\"][:-1], config[\"layers\"][1:])):\n",
    "            self.fc_layers.append(nn.Linear(in_size, out_size))\n",
    "            \n",
    "        self.affine_output = nn.Linear(in_features = config[\"layers\"][-1] + self.f_MF, out_features=1)\n",
    "        self.logistic = nn.Sigmoid()\n",
    "        \n",
    "    \n",
    "    def forward(self, u, i):\n",
    "        user_embedding_MF = self.embedding_user_MF(u)\n",
    "        item_embedding_MF = self.embedding_item_MF(i)\n",
    "        user_embedding_MLP = self.embedding_user_MLP(u)\n",
    "        item_embedding_MLP = self.embedding_item_MLP(i)\n",
    "        \n",
    "        # Multi-Layer Perceptron part\n",
    "        MLP_vector = torch.cat([user_embedding_MLP, item_embedding_MLP], dim=-1)\n",
    "        for idx, _ in enumerate(range(len(self.fc_layers))):\n",
    "            MLP_vector = self.fc_layers[idx](MLP_vector)\n",
    "            MLP_vector = nn.ReLU()(MLP_vector)\n",
    "        \n",
    "        # Martrix Factorization part\n",
    "        MF_vector = torch.mul(user_embedding_MF, item_embedding_MF)\n",
    "        \n",
    "        vector = torch.cat([MLP_vector, MF_vector], dim=-1)\n",
    "        logits = self.affine_output(vector)\n",
    "        rating = self.logistic(logits)\n",
    "        \n",
    "        return rating\n",
    "    \n",
    "    \n",
    "    def init_weight(self):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def load_pretrain_weights(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        load pretrained weights from GMF and MLP\n",
    "        \"\"\"\n",
    "        \n",
    "        config = self.config\n",
    "        config['latent_dim'] = config['latent_dim_MLP']\n",
    "        mlp_model = MLP(config)\n",
    "        mlp_model.cuda()\n",
    "        state_dict = torch.load('./checkpoints/MLP_Epoch100_HR0.5836_NDCG0.3299.model', map_location=lambda storage, loc: storage.cuda())\n",
    "        mlp_model.load_state_dict(state_dict)\n",
    "        self.embedding_user_MLP.weight.data = mlp_model.embedding_user.weight.data\n",
    "        self.embedding_item_MLP.weight.data = mlp_model.embedding_item.weight.data\n",
    "        for idx in range(len(self.fc_layers)):\n",
    "            self.fc_layers[idx].weight.data = mlp_model.fc_layers[idx].weight.data\n",
    "        \n",
    "        config[\"latent_dim\"] = config[\"latent_dim_MF\"]\n",
    "        gmf_model = GMF(config)\n",
    "        gmf_model.cuda()\n",
    "        state_dict = torch.load('./checkpoints/GMF_Epoch40_HR0.6399_NDCG0.3704.model', map_location=lambda storage, loc: storage.cuda())\n",
    "        gmf_model.load_state_dict(state_dict)\n",
    "        self.embedding_user_MF.weight.data = gmf_model.embedding_user.weight.data\n",
    "        self.embedding_item_MF.weight.data = gmf_model.embedding_item.weight.data\n",
    "        \n",
    "        \n",
    "        self.affine_output.weight.data = 0.5 * torch.cat([mlp_model.affine_output.weight.data, gmf_model.affine_output.weight.data], dim=-1)\n",
    "        self.affine_output.bias.data = 0.5 * (mlp_model.affine_output.bias.data + gmf_model.affine_output.bias.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "First, train GMF and MLP with random initializations until convergence  \n",
    "For training GMF, and MLP from scratch, we adopt the Adaptive Moment Estimation (Adam)  \n",
    "  \n",
    "Then use their model parameters as the initialization for the corresponding parts of NeuralMF's parameters  \n",
    "After feeding pre-trained parameters into NeuralMF, we optimize it with the vanilla SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboardX import SummaryWriter\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data\n",
    "ratings = dataframe.ratings\n",
    "sample_generator = data.SampleGenerator(data = ratings)\n",
    "test_data = sample_generator.evaluate_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train GMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(logdir=\"runs/GMF_v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmf_config = {\"num_users\":6040,\n",
    "              \"num_items\":3706,\n",
    "              \"latent_dim\":16,\n",
    "              \"num_negative\":4, #increasing number of negatives -> converges in less epochs\n",
    "              \"batch_size\":1024\n",
    "             }\n",
    "\n",
    "\n",
    "model = GMF(gmf_config).cuda()\n",
    "loss_function = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use tensorboard :  \n",
    "tensorboard --logdir=runs/ --host=0.0.0.0 --port=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Evluating Epoch 1] HR = 0.0970, NDCG = 0.0447 time = 115.0248sec\n",
      "[Evluating Epoch 2] HR = 0.0980, NDCG = 0.0451 time = 115.8239sec\n",
      "[Evluating Epoch 3] HR = 0.1510, NDCG = 0.0714 time = 114.9313sec\n",
      "[Evluating Epoch 4] HR = 0.3366, NDCG = 0.1792 time = 113.7466sec\n",
      "[Evluating Epoch 5] HR = 0.4053, NDCG = 0.2217 time = 114.0880sec\n",
      "[Evluating Epoch 6] HR = 0.4329, NDCG = 0.2375 time = 113.9665sec\n",
      "[Evluating Epoch 7] HR = 0.4469, NDCG = 0.2460 time = 115.1512sec\n",
      "[Evluating Epoch 8] HR = 0.4563, NDCG = 0.2500 time = 115.0018sec\n",
      "[Evluating Epoch 9] HR = 0.4705, NDCG = 0.2615 time = 113.6703sec\n",
      "[Evluating Epoch 10] HR = 0.4896, NDCG = 0.2721 time = 114.4799sec\n",
      "[Evluating Epoch 11] HR = 0.5055, NDCG = 0.2805 time = 114.6054sec\n",
      "[Evluating Epoch 12] HR = 0.5315, NDCG = 0.2952 time = 114.9387sec\n",
      "[Evluating Epoch 13] HR = 0.5513, NDCG = 0.3084 time = 116.4373sec\n",
      "[Evluating Epoch 14] HR = 0.5661, NDCG = 0.3191 time = 115.0321sec\n",
      "[Evluating Epoch 15] HR = 0.5868, NDCG = 0.3309 time = 114.3812sec\n",
      "[Evluating Epoch 16] HR = 0.5980, NDCG = 0.3374 time = 115.8627sec\n",
      "[Evluating Epoch 17] HR = 0.6101, NDCG = 0.3458 time = 117.3037sec\n",
      "[Evluating Epoch 18] HR = 0.6200, NDCG = 0.3517 time = 114.9813sec\n",
      "[Evluating Epoch 19] HR = 0.6272, NDCG = 0.3589 time = 114.7595sec\n",
      "[Evluating Epoch 20] HR = 0.6356, NDCG = 0.3638 time = 115.1062sec\n",
      "[Evluating Epoch 21] HR = 0.6434, NDCG = 0.3695 time = 115.0939sec\n",
      "[Evluating Epoch 22] HR = 0.6467, NDCG = 0.3728 time = 114.8780sec\n",
      "[Evluating Epoch 23] HR = 0.6495, NDCG = 0.3757 time = 114.8904sec\n",
      "[Evluating Epoch 24] HR = 0.6541, NDCG = 0.3797 time = 114.4135sec\n",
      "[Evluating Epoch 25] HR = 0.6604, NDCG = 0.3836 time = 114.1871sec\n",
      "[Evluating Epoch 26] HR = 0.6589, NDCG = 0.3851 time = 114.3337sec\n",
      "[Evluating Epoch 27] HR = 0.6639, NDCG = 0.3897 time = 113.3592sec\n",
      "[Evluating Epoch 28] HR = 0.6644, NDCG = 0.3894 time = 114.0003sec\n",
      "[Evluating Epoch 29] HR = 0.6662, NDCG = 0.3914 time = 114.4719sec\n",
      "[Evluating Epoch 30] HR = 0.6642, NDCG = 0.3925 time = 113.6375sec\n",
      "[Evluating Epoch 31] HR = 0.6677, NDCG = 0.3936 time = 114.8644sec\n",
      "[Evluating Epoch 32] HR = 0.6722, NDCG = 0.3979 time = 115.4312sec\n",
      "[Evluating Epoch 33] HR = 0.6755, NDCG = 0.3988 time = 114.2555sec\n",
      "[Evluating Epoch 34] HR = 0.6714, NDCG = 0.3989 time = 115.2883sec\n",
      "[Evluating Epoch 35] HR = 0.6725, NDCG = 0.4005 time = 115.4673sec\n",
      "[Evluating Epoch 36] HR = 0.6743, NDCG = 0.4010 time = 114.7507sec\n",
      "[Evluating Epoch 37] HR = 0.6770, NDCG = 0.4037 time = 115.2059sec\n",
      "[Evluating Epoch 38] HR = 0.6770, NDCG = 0.4030 time = 115.4212sec\n",
      "[Evluating Epoch 39] HR = 0.6791, NDCG = 0.4045 time = 114.3792sec\n",
      "[Evluating Epoch 40] HR = 0.6773, NDCG = 0.4050 time = 115.4187sec\n",
      "[Evluating Epoch 41] HR = 0.6793, NDCG = 0.4047 time = 115.0889sec\n",
      "[Evluating Epoch 42] HR = 0.6808, NDCG = 0.4059 time = 114.3698sec\n",
      "[Evluating Epoch 43] HR = 0.6806, NDCG = 0.4053 time = 115.4088sec\n",
      "[Evluating Epoch 44] HR = 0.6844, NDCG = 0.4086 time = 114.1547sec\n",
      "[Evluating Epoch 45] HR = 0.6810, NDCG = 0.4078 time = 112.9990sec\n",
      "[Evluating Epoch 46] HR = 0.6811, NDCG = 0.4080 time = 113.9280sec\n",
      "[Evluating Epoch 47] HR = 0.6810, NDCG = 0.4067 time = 114.1022sec\n",
      "[Evluating Epoch 48] HR = 0.6811, NDCG = 0.4081 time = 113.4538sec\n",
      "[Evluating Epoch 49] HR = 0.6808, NDCG = 0.4072 time = 113.4179sec\n",
      "[Evluating Epoch 50] HR = 0.6813, NDCG = 0.4084 time = 114.1978sec\n",
      "[Evluating Epoch 51] HR = 0.6811, NDCG = 0.4097 time = 112.4186sec\n",
      "[Evluating Epoch 52] HR = 0.6869, NDCG = 0.4114 time = 113.4950sec\n",
      "[Evluating Epoch 53] HR = 0.6848, NDCG = 0.4104 time = 113.8795sec\n",
      "[Evluating Epoch 54] HR = 0.6854, NDCG = 0.4106 time = 113.3523sec\n",
      "[Evluating Epoch 55] HR = 0.6864, NDCG = 0.4108 time = 113.8989sec\n",
      "[Evluating Epoch 56] HR = 0.6869, NDCG = 0.4100 time = 115.3037sec\n",
      "[Evluating Epoch 57] HR = 0.6882, NDCG = 0.4108 time = 114.3242sec\n",
      "[Evluating Epoch 58] HR = 0.6869, NDCG = 0.4114 time = 114.2487sec\n",
      "[Evluating Epoch 59] HR = 0.6866, NDCG = 0.4117 time = 114.4709sec\n",
      "[Evluating Epoch 60] HR = 0.6849, NDCG = 0.4111 time = 113.8192sec\n",
      "[Evluating Epoch 61] HR = 0.6869, NDCG = 0.4116 time = 114.1190sec\n",
      "[Evluating Epoch 62] HR = 0.6877, NDCG = 0.4120 time = 117.2656sec\n",
      "[Evluating Epoch 63] HR = 0.6863, NDCG = 0.4122 time = 113.6174sec\n",
      "[Evluating Epoch 64] HR = 0.6853, NDCG = 0.4119 time = 113.7041sec\n",
      "[Evluating Epoch 65] HR = 0.6853, NDCG = 0.4124 time = 113.9032sec\n",
      "[Evluating Epoch 66] HR = 0.6858, NDCG = 0.4119 time = 113.9154sec\n",
      "[Evluating Epoch 67] HR = 0.6887, NDCG = 0.4130 time = 114.1041sec\n",
      "[Evluating Epoch 68] HR = 0.6887, NDCG = 0.4130 time = 114.1859sec\n",
      "[Evluating Epoch 69] HR = 0.6866, NDCG = 0.4118 time = 113.1180sec\n",
      "[Evluating Epoch 70] HR = 0.6886, NDCG = 0.4131 time = 114.1427sec\n",
      "[Evluating Epoch 71] HR = 0.6876, NDCG = 0.4135 time = 114.8393sec\n",
      "[Evluating Epoch 72] HR = 0.6864, NDCG = 0.4128 time = 117.2423sec\n",
      "[Evluating Epoch 73] HR = 0.6889, NDCG = 0.4143 time = 116.5722sec\n",
      "[Evluating Epoch 74] HR = 0.6921, NDCG = 0.4161 time = 117.6042sec\n",
      "[Evluating Epoch 75] HR = 0.6907, NDCG = 0.4156 time = 116.1658sec\n",
      "[Evluating Epoch 76] HR = 0.6911, NDCG = 0.4132 time = 115.4807sec\n",
      "[Evluating Epoch 77] HR = 0.6879, NDCG = 0.4142 time = 115.1476sec\n",
      "[Evluating Epoch 78] HR = 0.6901, NDCG = 0.4131 time = 114.1749sec\n",
      "[Evluating Epoch 79] HR = 0.6902, NDCG = 0.4123 time = 114.8007sec\n",
      "[Evluating Epoch 80] HR = 0.6887, NDCG = 0.4134 time = 115.2598sec\n",
      "[Evluating Epoch 81] HR = 0.6891, NDCG = 0.4122 time = 114.3990sec\n",
      "[Evluating Epoch 82] HR = 0.6884, NDCG = 0.4125 time = 115.1596sec\n",
      "[Evluating Epoch 83] HR = 0.6886, NDCG = 0.4123 time = 115.3705sec\n",
      "[Evluating Epoch 84] HR = 0.6897, NDCG = 0.4126 time = 114.8331sec\n",
      "[Evluating Epoch 85] HR = 0.6912, NDCG = 0.4124 time = 118.1870sec\n",
      "[Evluating Epoch 86] HR = 0.6912, NDCG = 0.4131 time = 113.6089sec\n",
      "[Evluating Epoch 87] HR = 0.6896, NDCG = 0.4135 time = 112.0230sec\n",
      "[Evluating Epoch 88] HR = 0.6917, NDCG = 0.4150 time = 113.1123sec\n",
      "[Evluating Epoch 89] HR = 0.6882, NDCG = 0.4125 time = 113.2743sec\n",
      "[Evluating Epoch 90] HR = 0.6907, NDCG = 0.4127 time = 112.4757sec\n",
      "[Evluating Epoch 91] HR = 0.6916, NDCG = 0.4132 time = 112.9404sec\n",
      "[Evluating Epoch 92] HR = 0.6882, NDCG = 0.4129 time = 113.1404sec\n",
      "[Evluating Epoch 93] HR = 0.6881, NDCG = 0.4137 time = 112.0073sec\n",
      "[Evluating Epoch 94] HR = 0.6919, NDCG = 0.4144 time = 112.6968sec\n",
      "[Evluating Epoch 95] HR = 0.6907, NDCG = 0.4134 time = 112.9485sec\n",
      "[Evluating Epoch 96] HR = 0.6937, NDCG = 0.4148 time = 112.7757sec\n",
      "[Evluating Epoch 97] HR = 0.6882, NDCG = 0.4141 time = 112.6989sec\n",
      "[Evluating Epoch 98] HR = 0.6914, NDCG = 0.4148 time = 113.0478sec\n",
      "[Evluating Epoch 99] HR = 0.6925, NDCG = 0.4146 time = 112.4094sec\n",
      "[Evluating Epoch 100] HR = 0.6935, NDCG = 0.4157 time = 113.5654sec\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "# epoch\n",
    "for epoch_id in range(1, num_epochs + 1):\n",
    "    train_loader = sample_generator.instance_a_train_loader(gmf_config[\"num_negative\"], gmf_config[\"batch_size\"])\n",
    "    start_epoch = timer()\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        user, item, rating = batch[0], batch[1], batch[2]\n",
    "        user, item, rating = user.cuda(), item.cuda(), rating.float().cuda()\n",
    "        \n",
    "        # mini-batch update\n",
    "        optimizer.zero_grad()\n",
    "        prediction = model(user, item)\n",
    "        loss = loss_function(prediction.view(-1), rating)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    model.eval()\n",
    "    hit_ratio, ndcg = evaluate.cal_metrics(model, test_data)\n",
    "    \n",
    "    writer.add_scalar(\"loss/ Train_loss\", total_loss, epoch_id)\n",
    "    writer.add_scalar(\"performace/HitRate\", hit_ratio, epoch_id)\n",
    "    writer.add_scalar(\"performace/NDCG\", ndcg, epoch_id)\n",
    "    print('[Evluating Epoch {}] HR = {:.4f}, NDCG = {:.4f} time = {:.4f}sec'.format(epoch_id, hit_ratio, ndcg, timer()-start_epoch))\n",
    "    torch.save(model.state_dict(), \"checkpoints/GMF_v2_Epoch{}_HR{:.4f}_NDCG{:.4f}.model\".format(epoch_id, hit_ratio, ndcg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(logdir=\"runs/MLP_v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_config = {\"num_users\":6040,\n",
    "              \"num_items\":3706,\n",
    "              \"latent_dim\":16,\n",
    "              \"layers\":[32,16,8],\n",
    "              \"num_negative\":4,\n",
    "              \"batch_size\":1024\n",
    "             }\n",
    "\n",
    "\n",
    "model = MLP(mlp_config).cuda()\n",
    "loss_function = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "# epoch\n",
    "for epoch_id in range(1, num_epochs + 1):\n",
    "    \n",
    "    train_loader = sample_generator.instance_a_train_loader(mlp_config[\"num_negative\"], mlp_config[\"batch_size\"])\n",
    "    start_epoch = timer()\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        user, item, rating = batch[0], batch[1], batch[2]\n",
    "        user, item, rating = user.cuda(), item.cuda(), rating.float().cuda()\n",
    "        \n",
    "        # mini-batch update\n",
    "        optimizer.zero_grad()\n",
    "        prediction = model(user, item)\n",
    "        loss = loss_function(prediction.view(-1), rating)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    model.eval()\n",
    "    hit_ratio, ndcg = evaluate.cal_metrics(model, test_data)\n",
    "    \n",
    "    writer.add_scalar(\"loss/ Train_loss\", total_loss, epoch_id)\n",
    "    writer.add_scalar(\"performace/HitRate\", hit_ratio, epoch_id)\n",
    "    writer.add_scalar(\"performace/NDCG\", ndcg, epoch_id)\n",
    "    print('[Evluating Epoch {}] HR = {:.4f}, NDCG = {:.4f} time = {:.4f}sec'.format(epoch_id, hit_ratio, ndcg, timer()-start_epoch))\n",
    "    torch.save(model.state_dict(), \"checkpoints/MLP_Epoch{}_HR{:.4f}_NDCG{:.4f}.model\".format(epoch_id, hit_ratio, ndcg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train NeuralMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(logdir=\"runs/NeuMF_v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neumf_config = {\"num_users\":6040,\n",
    "              \"num_items\":3706,\n",
    "              \"latent_dim_MF\":16,\n",
    "              \"latent_dim_MLP\":16,\n",
    "              \"layers\":[32,16,8],\n",
    "              \"num_negative\":4,\n",
    "              \"batch_size\":1024\n",
    "             }\n",
    "\n",
    "\n",
    "model = NeuMF(neumf_config).cuda()\n",
    "model.load_pretrain_weights()\n",
    "loss_function = nn.BCELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "# epoch\n",
    "for epoch_id in range(1, num_epochs + 1):\n",
    "    \n",
    "    train_loader = sample_generator.instance_a_train_loader(neumf_config[\"num_negative\"], neumf_config[\"batch_size\"])\n",
    "    start_epoch = timer()\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        user, item, rating = batch[0], batch[1], batch[2]\n",
    "        user, item, rating = user.cuda(), item.cuda(), rating.float().cuda()\n",
    "        \n",
    "        # mini-batch update\n",
    "        optimizer.zero_grad()\n",
    "        prediction = model(user, item)\n",
    "        loss = loss_function(prediction.view(-1), rating)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    model.eval()\n",
    "    hit_ratio, ndcg = evaluate.cal_metrics(model, test_data)\n",
    "    \n",
    "    writer.add_scalar(\"loss/ Train_loss\", total_loss, epoch_id)\n",
    "    writer.add_scalar(\"performace/HitRate\", hit_ratio, epoch_id)\n",
    "    writer.add_scalar(\"performace/NDCG\", ndcg, epoch_id)\n",
    "    print('[Evluating Epoch {}] HR = {:.4f}, NDCG = {:.4f} time = {:.4f}sec'.format(epoch_id, hit_ratio, ndcg, timer()-start_epoch))\n",
    "    torch.save(model.state_dict(), \"checkpoints/NeuMF_Epoch{}_HR{:.4f}_NDCG{:.4f}.model\".format(epoch_id, hit_ratio, ndcg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mf_v1",
   "language": "python",
   "name": "mf_v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
