{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml1m_dir = 'ml-1m/ratings.dat'\n",
    "ml1m_rating = pd.read_csv(ml1m_dir, sep='::', header=None, names=['uid', 'mid', 'rating', 'timestamp'],  engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = ml1m_rating[['uid']].drop_duplicates().reindex()\n",
    "user_id['userId'] = np.arange(len(user_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_id = ml1m_rating[['mid']].drop_duplicates()\n",
    "item_id['itemId'] = np.arange(len(item_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml1m_rating = pd.merge(ml1m_rating, user_id, on=['uid'], how='left')\n",
    "ml1m_rating = pd.merge(ml1m_rating, item_id, on=['mid'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml1m_rating = ml1m_rating[['userId', 'itemId', 'rating', 'timestamp']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_pool = set(ml1m_rating['userId'].unique())\n",
    "item_pool = set(ml1m_rating['itemId'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to binary data\n",
    "ratings = deepcopy(ml1m_rating)\n",
    "ratings['rating'][ratings['rating'] > 0] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train, test data\n",
    "# we adopt the leave-one-out evaluation\n",
    "# For each user, we held-out her latest interaction as the test set and utilized the rematining data for training\n",
    "ratings['rank_latest'] = ratings.groupby(['userId'])['timestamp'].rank(method='first', ascending=False)\n",
    "test = ratings[ratings['rank_latest'] == 1]\n",
    "train = ratings[ratings['rank_latest'] > 1]\n",
    "\n",
    "train = train[['userId', 'itemId', 'rating']]\n",
    "test = test[['userId', 'itemId', 'rating']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact_status = ratings.groupby(\"userId\")[\"itemId\"].apply(set).reset_index().rename(columns={\"itemId\":\"interacted_items\"})\n",
    "interact_status['negative_items'] = interact_status['interacted_items'].apply(lambda x: item_pool - x)\n",
    "# we uniformly sample negative instances from unobserved interactions in each iteration\n",
    "# and control the sampling ratio w.r.t. the number of observed interations\n",
    "interact_status['negative_samples'] = interact_status['negative_items'].apply(lambda x: random.sample(x, 99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset\n",
    "# https://wikidocs.net/57165\n",
    "class RatingDataset(Dataset):\n",
    "    \"\"\"\n",
    "    torch.utils.data.Dataset 상속\n",
    "    \"\"\"\n",
    "    def __init__(self, user_tensor, item_tensor, target_tensor):\n",
    "        self.user_tensor = user_tensor\n",
    "        self.item_tensor = item_tensor\n",
    "        self.target_tensor = target_tensor\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.user_tensor.size(0)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.user_tensor[index], self.item_tensor[index], self.target_tensor[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users, items, ratings = [], [], []\n",
    "train_ratings = pd.merge(train, interact_status)\n",
    "# sample 10 negative items\n",
    "train_ratings[\"negatives\"] = train_ratings[\"negative_items\"].apply(lambda x : random.sample(x, 10))\n",
    "train_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in train_ratings.itertuples():\n",
    "    users.append(int(row.userId))\n",
    "    items.append(int(row.itemId))\n",
    "    ratings.append(float(row.rating))\n",
    "    for i in range(num_negatives):\n",
    "        users.append(int(row.userId))\n",
    "        items.append(int(row.negatives[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RecSys_v1",
   "language": "python",
   "name": "recsys_v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
