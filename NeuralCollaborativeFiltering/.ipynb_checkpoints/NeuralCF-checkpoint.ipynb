{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of Neural Collaborative Filtering\n",
    "https://arxiv.org/abs/1708.05031"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataframe\n",
    "import data\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select GPU Number\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if cuda available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "torch.manual_seed(315)\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.manual_seed_all(912)\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalized Matrix Factorization\n",
    "MF can be interpreted as a special case of our NCF framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GMF(nn.Module):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super(GMF, self).__init__() # run nn.Module.__init__()\n",
    "        self.num_users = config[\"num_users\"]\n",
    "        self.num_items = config[\"num_items\"]\n",
    "        self.f = config[\"latent_dim\"]\n",
    "        \n",
    "        # Embedding Layer\n",
    "        self.embedding_user = nn.Embedding(num_embeddings = self.num_users, embedding_dim = self.f)\n",
    "        self.embedding_item = nn.Embedding(num_embeddings = self.num_items, embedding_dim = self.f)\n",
    "        \n",
    "        # One layer\n",
    "        self.affine_output = nn.Linear(in_features = self.f, out_features = 1)\n",
    "        self.logistic = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, u, i):\n",
    "        \n",
    "        user_embedding = self.embedding_user(u)\n",
    "        item_embedding = self.embedding_item(i)\n",
    "        product = torch.mul(user_embedding, item_embedding) # element-wise product\n",
    "        logits = self.affine_output(product)\n",
    "        rating = self.logistic(logits)\n",
    "        \n",
    "        return rating\n",
    "    \n",
    "    def init_weight(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super(MLP, self).__init__()\n",
    "        self.num_users = config[\"num_users\"]\n",
    "        self.num_items = config[\"num_items\"]\n",
    "        self.f = config[\"latent_dim\"]\n",
    "        \n",
    "        # Build Layers\n",
    "        ## Embedding Layer\n",
    "        self.embedding_user = nn.Embedding(num_embeddings = self.num_users, embedding_dim = self.f)\n",
    "        self.embedding_item = nn.Embedding(num_embeddings = self.num_items, embedding_dim = self.f)\n",
    "        \n",
    "        ## Fully Connected Layer\n",
    "        self.fc_layers = nn.ModuleList() # holds submodules in a list\n",
    "        for idx, (in_size, out_size) in enumerate(zip(config[\"layers\"][:-1], config[\"layers\"][1:])):\n",
    "            self.fc_layers.append(nn.Linear(in_size, out_size))\n",
    "        \n",
    "        ## Final Layer\n",
    "        self.affine_output = nn.Linear(in_features = config[\"layers\"][-1], out_features = 1)\n",
    "        self.logistic = nn.Sigmoid()\n",
    "        \n",
    "    \n",
    "    def forward(self, u, i):\n",
    "        \n",
    "        user_embedding = self.embedding_user(u)\n",
    "        item_embedding = self.embedding_item(i)\n",
    "        vector = torch.cat([user_embedding, item_embedding], dim=-1) # concatenate user, item\n",
    "        for idx, _ in enumerate(range(len(self.fc_layers))):\n",
    "            vector = self.fc_layers[idx](vector)\n",
    "            vector = nn.ReLU()(vector)\n",
    "        logits = self.affine_output(vector)\n",
    "        rating = self.logistic(logits)\n",
    "        \n",
    "        return rating\n",
    "    \n",
    "    \n",
    "    def init_weight(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Matrix Factorization\n",
    "We allow GMF and MLP to learn separate embeddings, and combine the two models by concatenating their last hidden layer  \n",
    "This model combines the linearity of MF and non-linearity of Deep Neural Networks for modelling user-item latent structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuMF(nn.Module):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super(NeuMF, self).__init__()\n",
    "        self.config = config\n",
    "        self.num_users = config[\"num_users\"]\n",
    "        self.num_items = config[\"num_items\"]\n",
    "        self.f_MF = config[\"latent_dim_MF\"]\n",
    "        self.f_MLP = config[\"latent_dim_MLP\"]\n",
    "        \n",
    "        self.embedding_user_MF = nn.Embedding(num_embeddings = self.num_users, embedding_dim = self.f_MF)\n",
    "        self.embedding_item_MF = nn.Embedding(num_embeddings = self.num_items, embedding_dim = self.f_MF)\n",
    "        self.embedding_user_MLP = nn.Embedding(num_embeddings = self.num_users, embedding_dim = self.f_MLP)\n",
    "        self.embedding_item_MLP = nn.Embedding(num_embeddings = self.num_items, embedding_dim = self.f_MLP)\n",
    "        \n",
    "        self.fc_layers = nn.ModuleList()\n",
    "        for idx, (in_size, out_size) in enumerate(zip(config[\"layers\"][:-1], config[\"layers\"][1:])):\n",
    "            self.fc_layers.append(nn.Linear(in_size, out_size))\n",
    "            \n",
    "        self.affine_output = nn.Linear(in_features = config[\"layers\"][-1] + self.f_MF, out_features=1)\n",
    "        self.logistic = nn.Sigmoid()\n",
    "        \n",
    "    \n",
    "    def forward(self, u, i):\n",
    "        user_embedding_MF = self.embedding_user_MF(u)\n",
    "        item_embedding_MF = self.embedding_item_MF(i)\n",
    "        user_embedding_MLP = self.embedding_user_MLP(u)\n",
    "        item_embedding_MLP = self.embedding_item_MLP(i)\n",
    "        \n",
    "        # Multi-Layer Perceptron part\n",
    "        MLP_vector = torch.cat([user_embedding_MLP, item_embedding_MLP], dim=-1)\n",
    "        for idx, _ in enumerate(range(len(self.fc_layers))):\n",
    "            MLP_vector = self.fc_layers[idx](MLP_vector)\n",
    "            MLP_vector = nn.ReLU()(MLP_vector)\n",
    "        \n",
    "        # Martrix Factorization part\n",
    "        MF_vector = torch.mul(user_embedding_MF, item_embedding_MF)\n",
    "        \n",
    "        vector = torch.cat([MLP_vector, MF_vector], dim=-1)\n",
    "        logits = self.affine_output(vector)\n",
    "        rating = self.logistic(logits)\n",
    "        \n",
    "        return rating\n",
    "    \n",
    "    \n",
    "    def init_weight(self):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def load_pretrain_weights(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        load pretrained weights from GMF and MLP\n",
    "        \"\"\"\n",
    "        \n",
    "        config = self.config\n",
    "        config['latent_dim'] = config['latent_dim_MLP']\n",
    "        mlp_model = MLP(config)\n",
    "        mlp_model.cuda()\n",
    "        state_dict = torch.load('./checkpoints/MLP_Epoch100_HR0.5836_NDCG0.3299.model', map_location=lambda storage, loc: storage.cuda())\n",
    "        mlp_model.load_state_dict(state_dict)\n",
    "        self.embedding_user_MLP.weight.data = mlp_model.embedding_user.weight.data\n",
    "        self.embedding_item_MLP.weight.data = mlp_model.embedding_item.weight.data\n",
    "        for idx in range(len(self.fc_layers)):\n",
    "            self.fc_layers[idx].weight.data = mlp_model.fc_layers[idx].weight.data\n",
    "        \n",
    "        config[\"latent_dim\"] = config[\"latent_dim_MF\"]\n",
    "        gmf_model = GMF(config)\n",
    "        gmf_model.cuda()\n",
    "        state_dict = torch.load('./checkpoints/GMF_Epoch40_HR0.6399_NDCG0.3704.model', map_location=lambda storage, loc: storage.cuda())\n",
    "        gmf_model.load_state_dict(state_dict)\n",
    "        self.embedding_user_MF.weight.data = gmf_model.embedding_user.weight.data\n",
    "        self.embedding_item_MF.weight.data = gmf_model.embedding_item.weight.data\n",
    "        \n",
    "        \n",
    "        self.affine_output.weight.data = 0.5 * torch.cat([mlp_model.affine_output.weight.data, gmf_model.affine_output.weight.data], dim=-1)\n",
    "        self.affine_output.bias.data = 0.5 * (mlp_model.affine_output.bias.data + gmf_model.affine_output.bias.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "First, train GMF and MLP with random initializations until convergence  \n",
    "For training GMF, and MLP from scratch, we adopt the Adaptive Moment Estimation (Adam)  \n",
    "  \n",
    "Then use their model parameters as the initialization for the corresponding parts of NeuralMF's parameters  \n",
    "After feeding pre-trained parameters into NeuralMF, we optimize it with the vanilla SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboardX import SummaryWriter\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data\n",
    "ratings = dataframe.ratings\n",
    "sample_generator = data.SampleGenerator(data = ratings)\n",
    "test_data = sample_generator.evaluate_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train GMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(logdir=\"runs/GMF_v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmf_config = {\"num_users\":6040,\n",
    "              \"num_items\":3706,\n",
    "              \"latent_dim\":16,\n",
    "              \"num_negative\":4,\n",
    "              \"batch_size\":1024\n",
    "             }\n",
    "\n",
    "\n",
    "model = GMF(gmf_config).cuda()\n",
    "loss_function = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use tensorboard :  \n",
    "tensorboard --logdir=runs/ --host=0.0.0.0 --port=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Evluating Epoch 1] HR = 0.1022, NDCG = 0.0453 time = 214.2537sec\n",
      "[Evluating Epoch 2] HR = 0.1041, NDCG = 0.0468 time = 222.6292sec\n",
      "[Evluating Epoch 3] HR = 0.1005, NDCG = 0.0455 time = 221.8903sec\n",
      "[Evluating Epoch 4] HR = 0.2950, NDCG = 0.1580 time = 221.9160sec\n",
      "[Evluating Epoch 5] HR = 0.4111, NDCG = 0.2264 time = 221.8846sec\n",
      "[Evluating Epoch 6] HR = 0.4445, NDCG = 0.2459 time = 221.5675sec\n",
      "[Evluating Epoch 7] HR = 0.4702, NDCG = 0.2609 time = 222.9268sec\n",
      "[Evluating Epoch 8] HR = 0.4929, NDCG = 0.2727 time = 224.3428sec\n",
      "[Evluating Epoch 9] HR = 0.5195, NDCG = 0.2883 time = 223.7368sec\n",
      "[Evluating Epoch 10] HR = 0.5419, NDCG = 0.3026 time = 223.0527sec\n",
      "[Evluating Epoch 11] HR = 0.5649, NDCG = 0.3176 time = 221.8626sec\n",
      "[Evluating Epoch 12] HR = 0.5806, NDCG = 0.3287 time = 222.0894sec\n",
      "[Evluating Epoch 13] HR = 0.5957, NDCG = 0.3386 time = 223.7962sec\n",
      "[Evluating Epoch 14] HR = 0.6093, NDCG = 0.3469 time = 222.3987sec\n",
      "[Evluating Epoch 15] HR = 0.6164, NDCG = 0.3533 time = 221.8002sec\n",
      "[Evluating Epoch 16] HR = 0.6267, NDCG = 0.3575 time = 223.6002sec\n",
      "[Evluating Epoch 17] HR = 0.6310, NDCG = 0.3608 time = 223.6617sec\n",
      "[Evluating Epoch 18] HR = 0.6348, NDCG = 0.3646 time = 223.4046sec\n",
      "[Evluating Epoch 19] HR = 0.6387, NDCG = 0.3667 time = 221.4696sec\n",
      "[Evluating Epoch 20] HR = 0.6343, NDCG = 0.3656 time = 223.4854sec\n",
      "[Evluating Epoch 21] HR = 0.6364, NDCG = 0.3672 time = 222.9542sec\n",
      "[Evluating Epoch 22] HR = 0.6404, NDCG = 0.3686 time = 223.6233sec\n",
      "[Evluating Epoch 23] HR = 0.6373, NDCG = 0.3698 time = 222.2663sec\n",
      "[Evluating Epoch 24] HR = 0.6376, NDCG = 0.3697 time = 221.7398sec\n",
      "[Evluating Epoch 25] HR = 0.6396, NDCG = 0.3705 time = 223.9650sec\n",
      "[Evluating Epoch 26] HR = 0.6401, NDCG = 0.3705 time = 224.2905sec\n",
      "[Evluating Epoch 27] HR = 0.6361, NDCG = 0.3685 time = 224.3024sec\n",
      "[Evluating Epoch 28] HR = 0.6358, NDCG = 0.3698 time = 224.0708sec\n",
      "[Evluating Epoch 29] HR = 0.6366, NDCG = 0.3693 time = 223.2412sec\n",
      "[Evluating Epoch 30] HR = 0.6387, NDCG = 0.3718 time = 221.6072sec\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-c1222aefc591>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mhit_ratio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndcg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcal_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loss/ Train_loss\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/RecSys_paper/NeuralCollaborativeFiltering/evaluate.py\u001b[0m in \u001b[0;36mcal_metrics\u001b[0;34m(model, test_data)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mtest_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mneg_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m99\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mconcat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg_pred\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "# epoch\n",
    "for epoch_id in range(1, num_epochs + 1):\n",
    "    train_loader = sample_generator.instance_a_train_loader(gmf_config[\"num_negative\"], gmf_config[\"batch_size\"])\n",
    "    start_epoch = timer()\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        user, item, rating = batch[0], batch[1], batch[2]\n",
    "        user, item, rating = user.cuda(), item.cuda(), rating.float().cuda()\n",
    "        \n",
    "        # mini-batch update\n",
    "        optimizer.zero_grad()\n",
    "        prediction = model(user, item)\n",
    "        loss = loss_function(prediction.view(-1), rating)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    model.eval()\n",
    "    hit_ratio, ndcg = evaluate.cal_metrics(model, test_data)\n",
    "    \n",
    "    writer.add_scalar(\"loss/ Train_loss\", total_loss, epoch_id)\n",
    "    writer.add_scalar(\"performace/HitRate\", hit_ratio, epoch_id)\n",
    "    writer.add_scalar(\"performace/NDCG\", ndcg, epoch_id)\n",
    "    print('[Evluating Epoch {}] HR = {:.4f}, NDCG = {:.4f} time = {:.4f}sec'.format(epoch_id, hit_ratio, ndcg, timer()-start_epoch))\n",
    "    torch.save(model.state_dict(), \"checkpoints/GMF_v2_Epoch{}_HR{:.4f}_NDCG{:.4f}.model\".format(epoch_id, hit_ratio, ndcg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(logdir=\"runs/MLP_v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_config = {\"num_users\":6040,\n",
    "              \"num_items\":3706,\n",
    "              \"latent_dim\":16,\n",
    "              \"layers\":[32,16,8],\n",
    "              \"num_negative\":4,\n",
    "              \"batch_size\":1024\n",
    "             }\n",
    "\n",
    "\n",
    "model = MLP(mlp_config).cuda()\n",
    "loss_function = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "# epoch\n",
    "for epoch_id in range(1, num_epochs + 1):\n",
    "    \n",
    "    train_loader = sample_generator.instance_a_train_loader(mlp_config[\"num_negative\"], mlp_config[\"batch_size\"])\n",
    "    start_epoch = timer()\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        user, item, rating = batch[0], batch[1], batch[2]\n",
    "        user, item, rating = user.cuda(), item.cuda(), rating.float().cuda()\n",
    "        \n",
    "        # mini-batch update\n",
    "        optimizer.zero_grad()\n",
    "        prediction = model(user, item)\n",
    "        loss = loss_function(prediction.view(-1), rating)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    model.eval()\n",
    "    hit_ratio, ndcg = evaluate.cal_metrics(model, test_data)\n",
    "    \n",
    "    writer.add_scalar(\"loss/ Train_loss\", total_loss, epoch_id)\n",
    "    writer.add_scalar(\"performace/HitRate\", hit_ratio, epoch_id)\n",
    "    writer.add_scalar(\"performace/NDCG\", ndcg, epoch_id)\n",
    "    print('[Evluating Epoch {}] HR = {:.4f}, NDCG = {:.4f} time = {:.4f}sec'.format(epoch_id, hit_ratio, ndcg, timer()-start_epoch))\n",
    "    torch.save(model.state_dict(), \"checkpoints/MLP_Epoch{}_HR{:.4f}_NDCG{:.4f}.model\".format(epoch_id, hit_ratio, ndcg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train NeuralMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(logdir=\"runs/NeuMF_v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neumf_config = {\"num_users\":6040,\n",
    "              \"num_items\":3706,\n",
    "              \"latent_dim_MF\":16,\n",
    "              \"latent_dim_MLP\":16,\n",
    "              \"layers\":[32,16,8],\n",
    "              \"num_negative\":4,\n",
    "              \"batch_size\":1024\n",
    "             }\n",
    "\n",
    "\n",
    "model = NeuMF(neumf_config).cuda()\n",
    "model.load_pretrain_weights()\n",
    "loss_function = nn.BCELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "# epoch\n",
    "for epoch_id in range(1, num_epochs + 1):\n",
    "    \n",
    "    train_loader = sample_generator.instance_a_train_loader(neumf_config[\"num_negative\"], neumf_config[\"batch_size\"])\n",
    "    start_epoch = timer()\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        user, item, rating = batch[0], batch[1], batch[2]\n",
    "        user, item, rating = user.cuda(), item.cuda(), rating.float().cuda()\n",
    "        \n",
    "        # mini-batch update\n",
    "        optimizer.zero_grad()\n",
    "        prediction = model(user, item)\n",
    "        loss = loss_function(prediction.view(-1), rating)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    model.eval()\n",
    "    hit_ratio, ndcg = evaluate.cal_metrics(model, test_data)\n",
    "    \n",
    "    writer.add_scalar(\"loss/ Train_loss\", total_loss, epoch_id)\n",
    "    writer.add_scalar(\"performace/HitRate\", hit_ratio, epoch_id)\n",
    "    writer.add_scalar(\"performace/NDCG\", ndcg, epoch_id)\n",
    "    print('[Evluating Epoch {}] HR = {:.4f}, NDCG = {:.4f} time = {:.4f}sec'.format(epoch_id, hit_ratio, ndcg, timer()-start_epoch))\n",
    "    torch.save(model.state_dict(), \"checkpoints/NeuMF_Epoch{}_HR{:.4f}_NDCG{:.4f}.model\".format(epoch_id, hit_ratio, ndcg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mf_v1",
   "language": "python",
   "name": "mf_v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
