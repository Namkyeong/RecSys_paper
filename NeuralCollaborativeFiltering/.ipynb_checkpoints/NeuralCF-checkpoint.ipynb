{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of Neural Collaborative Filtering\n",
    "https://arxiv.org/abs/1708.05031"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "torch.manual_seed(315)\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.manual_seed_all(912)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalized Matrix Factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GMF(nn.Module):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super(GMF, self).__init__() # run nn.Module.__init__()\n",
    "        self.num_users = config[\"num_users\"]\n",
    "        self.num_items = config[\"num_items\"]\n",
    "        self.f = config[\"latent_dim\"]\n",
    "        \n",
    "        # Embedding Layer\n",
    "        self.embedding_user = nn.Embedding(num_embeddings = self.num_users, embedding_dim = self.f)\n",
    "        self.embedding_item = nn.Embedding(num_embeddings = self.num_items, embedding_dim = self.f)\n",
    "        \n",
    "        # One layer\n",
    "        self.affine_output = nn.Linear(in_features = self.f, out_features = 1)\n",
    "        self.logistic = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, u, i):\n",
    "        \n",
    "        user_embedding = self.embedding_user(u)\n",
    "        item_embedding = self.embedding_item(i)\n",
    "        product = torch.mul(user_embedding, item_embedding) # element-wise product\n",
    "        logits = self.affine_output(product)\n",
    "        rating = self.logistic(logits)\n",
    "        \n",
    "        return rating\n",
    "    \n",
    "    def init_weight(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super(MLP, self).__init__()\n",
    "        self.num_users = config[\"num_users\"]\n",
    "        self.num_items = config[\"num_items\"]\n",
    "        self.f = config[\"latent_dim\"]\n",
    "        \n",
    "        # Build Layers\n",
    "        ## Embedding Layer\n",
    "        self.embedding_user = nn.Embedding(num_embeddings = self.num_users, embedding_dim = self.f)\n",
    "        self.embedding_item = nn.Embedding(num_embeddings = self.num_items, embedding_dim = self.f)\n",
    "        \n",
    "        ## Fully Connected Layer\n",
    "        self.fc_layers = torch.nn.ModuleList() # holds submodules in a list\n",
    "        for idx, (insize, out_size) in enumerate(zip(config[\"layers\"][:-1], config[\"layers\"][1:])):\n",
    "            self.fc_layers.append(torch.nn.Linear(in_size, out_size))\n",
    "        \n",
    "        ## Final Layer\n",
    "        self.affine_output = torch.nn.Linear(in_features = config[\"layers\"][-1], out_features = 1)\n",
    "        self.logistic = torch.nn.Sigmoid\n",
    "        \n",
    "    \n",
    "    def forward(self, u, i):\n",
    "        \n",
    "        user_embedding = self.embedding_user(u)\n",
    "        item_embedding = self.embedding_item(i)\n",
    "        vector = torch.cat([user_embedding, item_embedding], dim=-1) # concatenate user, item\n",
    "        for idx, _ in enumerate(range(len(self.fc_layers))):\n",
    "            vector = self.fc_layers[idx](vector)\n",
    "            vector = torch.nn.ReLU()(vector)\n",
    "        logits = self.affine_output(vector)\n",
    "        rating = self.logistic(logits)\n",
    "        \n",
    "        return rating\n",
    "    \n",
    "    \n",
    "    def init_weight(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Matrix Factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuMF(nn.Module):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super(NeuMF, self).__init__()\n",
    "        self.num_users = config[\"num_users\"]\n",
    "        self.num_items = config[\"num_items\"]\n",
    "        self.f_MF = config[\"latent_dim_MF\"]\n",
    "        self.f_MLP = config[\"latent_dim_MLP\"]\n",
    "        \n",
    "        self.embedding_user_MF = nn.Embedding(num_embeddings = self.num_users, embedding_dim = self.f_MF)\n",
    "        self.embedding_item_MF = nn.Embedding(num_embeddings = self.num_items, embedding_dim = self.f_MF)\n",
    "        self.embedding_user_MLP = nn.Embedding(num_embeddings = self.num_users, embedding_dim = self.f_MLP)\n",
    "        self.embedding_item_MLP = nn.Embedding(num_embeddings = self.num_items, embedding_dim = self.f_MLP)\n",
    "        \n",
    "        self.fc_layers = nn.ModuleList()\n",
    "        for idx, (in_size, out_size) in enumerate(zip(config[\"layers\"][:-1], config[\"layers\"][1:])):\n",
    "            self.fc_layers.append(nn.Linear(in_size, out_size))\n",
    "            \n",
    "        self.affine_output = nn.Linear(in_features = config[\"layers\"][-1] + self.f_MF, out_features=1)\n",
    "        self.logistic = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, u, i):\n",
    "        user_embedding_MF = self.embedding_user_MF(u)\n",
    "        item_embedding_MF = self.embedding_item_MF(i)\n",
    "        user_embedding_MLP = self.embedding_user_MLP(u)\n",
    "        item_embedding_MLP = self.embedding_item_MLP(i)\n",
    "        \n",
    "        # Multi-Layer Perceptron part\n",
    "        MLP_vector = torch.cat([user_embedding_MLP, item_embedding_MLP], dim=-1)\n",
    "        for idx, _ in enumerate(range(len(self.fc_layers))):\n",
    "            MLP_vector = self.fc_layers[idx](MLP_vector)\n",
    "            MLP_vector = nn.ReLU()(MLP_vector)\n",
    "        \n",
    "        # Martrix Factorization part\n",
    "        MF_vector = torch.mul(user_embedding_MF, item_embedding_MF)\n",
    "        \n",
    "        vector = torch.cat([MLP_vector, MF_vector], dim=-1)\n",
    "        logits = self.affine_output(vector)\n",
    "        rating = self.logistic(logits)\n",
    "        \n",
    "        return rating\n",
    "    \n",
    "    def init_weight(self):\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mf_v1",
   "language": "python",
   "name": "mf_v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
