{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select GPU Number\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if cuda available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "torch.manual_seed(315)\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.manual_seed_all(912)\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = dataframe.train\n",
    "test = dataframe.test\n",
    "ratings = dataframe.ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_pool = set(train[\"userId\"].unique()) # 6040\n",
    "item_pool = set(train[\"itemId\"].unique()) # 3706\n",
    "interact_status = train.groupby(\"userId\")[\"itemId\"].apply(set).reset_index().rename(columns = {\"itemId\" : \"interacted_items\"})\n",
    "interact_status[\"negative_items\"] = interact_status[\"interacted_items\"].apply(lambda x: item_pool - x)\n",
    "train_loader = pd.merge(train, interact_status, on=\"userId\")\n",
    "train_loader[\"negatives\"] = train_loader[\"negative_items\"].apply(lambda x: random.sample(x, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "users, items, neg_items = [], [], []\n",
    "for row in train_loader.itertuples():\n",
    "    users.append(int(row.userId))\n",
    "    items.append(int(row.itemId))\n",
    "    neg_items.append(row.negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RatingDataset(Dataset):\n",
    "    \"\"\"\n",
    "    torch.utils.data.Dataset 상속\n",
    "    \"\"\"\n",
    "    def __init__(self, user_tensor, item_tensor, neg_item_list):\n",
    "        self.user_tensor = user_tensor\n",
    "        self.item_tensor = item_tensor\n",
    "        self.neg_items = neg_item_list\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.user_tensor.size(0)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.user_tensor[index], self.item_tensor[index], self.neg_items[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = RatingDataset(user_tensor = torch.LongTensor(users),\n",
    "                        item_tensor = torch.LongTensor(items),\n",
    "                        neg_item_list = torch.LongTensor(neg_items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create test set\n",
    "test_user, test_item = [], []\n",
    "for i in range(len(test)):\n",
    "    test_user.append(test[\"userId\"][i])\n",
    "    test_item.append(test[\"itemId\"][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CML(nn.Module):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        \n",
    "        super(CML, self).__init__()\n",
    "        self.config = config\n",
    "        self.num_users = config[\"num_users\"]\n",
    "        self.num_items = config[\"num_items\"]\n",
    "        self.latent_dim = config[\"latent_dim\"]\n",
    "        self.margin = config[\"margin\"]\n",
    "        self.lambda_c = config[\"lambda_c\"]\n",
    "        \n",
    "        self.U = torch.rand(self.num_users, self.latent_dim, requires_grad = True)\n",
    "        self.V = torch.rand(self.num_items, self.latent_dim, requires_grad = True)\n",
    "        \n",
    "        # self.user_embedding = nn.Embedding(self.num_users, self.latent_dim, max_norm = 1) # restrict norms\n",
    "        # self.item_embedding = nn.Embedding(self.num_items, self.latent_dim, max_norm = 1)\n",
    "        \n",
    "    \n",
    "    def distance_loss(self, i, j, k):\n",
    "        \"\"\"\n",
    "        compute distance loss\n",
    "        \"\"\"\n",
    "        user = self.U[i].view(len(i), 1, self.latent_dim)\n",
    "        item = self.V[j].view(len(i), 1, self.latent_dim)\n",
    "        neg_item = self.V[k]\n",
    "        # user = self.user_embedding(i).view(len(i), 1, self.latent_dim) # batchsize, X, latent_dim\n",
    "        # item = self.item_embedding(j).view(len(i), 1, self.latent_dim)\n",
    "        # neg_item = self.item_embedding(k)\n",
    "        d_ij = torch.cdist(user, item).view(-1, 1)**2 #(1024, 1)\n",
    "        d_ik = torch.cdist(user, neg_item).view(-1, 10)**2 #(1024, 10)\n",
    "        \n",
    "        metric = self.margin + d_ij - d_ik # (1024, 10)\n",
    "        loss = 0\n",
    "        for i in range(len(metric)):\n",
    "            temp_metric = metric[i][metric[i]>0]\n",
    "            rank_d_ij = 3676 * len(temp_metric) / 10\n",
    "            w_ij = np.log(rank_d_ij + 1)\n",
    "            loss +=  (w_ij * temp_metric).sum()\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def cov_loss(self):\n",
    "        \n",
    "        # self.U = self.user_embedding(torch.LongTensor([x for x in range(self.num_users)]).cuda())\n",
    "        # self.V = self.item_embedding(torch.LongTensor([x for x in range(self.num_items)]).cuda())\n",
    "        matrix = torch.cat([self.U, self.V])\n",
    "        n_rows = matrix.shape[0]\n",
    "        matrix = matrix - torch.mean(matrix, dim=0)\n",
    "        cov = torch.matmul(matrix.T, matrix) / n_rows\n",
    "        loss = (torch.linalg.norm(cov) - torch.linalg.norm(torch.diagonal(cov),2))/self.num_users\n",
    "        \n",
    "        return loss * self.lambda_c\n",
    "    \n",
    "    \n",
    "    def evaluate(self, train_user, train_item, test_user, test_item):\n",
    "        \n",
    "        # self.U = self.user_embedding(torch.LongTensor([x for x in range(self.num_users)]).cuda())\n",
    "        # self.V = self.item_embedding(torch.LongTensor([x for x in range(self.num_items)]).cuda())\n",
    "        x = torch.cdist(self.U, self.V)\n",
    "        for i, j in zip(train_user, train_item):\n",
    "            x[i, j] = 100\n",
    "        _, indices = x.topk(50, largest = False)\n",
    "        indices = indices.cpu().detach().numpy()\n",
    "        hit = 0\n",
    "        count = 0\n",
    "        for i in range(len(test_user)):\n",
    "            count += 1\n",
    "            if test_item[i] in indices[test_user[i]]:\n",
    "                hit += 1\n",
    "        \n",
    "        return hit/count\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "CML_config = {\n",
    "    \"num_users\" : 6040,\n",
    "    \"num_items\" : 3706,\n",
    "    \"latent_dim\" : 64,\n",
    "    \"margin\" : 0.5,\n",
    "    \"lambda_c\": 10\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CML(CML_config).cuda()\n",
    "optimizer = optim.Adagrad(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_loss = 14118812.7900, recall@50 = 0.1179, epoch_time = 362.5674sec\n",
      "total_loss = 12991575.8491, recall@50 = 0.1200, epoch_time = 359.3683sec\n",
      "total_loss = 12405828.6445, recall@50 = 0.1240, epoch_time = 338.4694sec\n",
      "total_loss = 11940024.3711, recall@50 = 0.1322, epoch_time = 355.2758sec\n",
      "total_loss = 11506672.5107, recall@50 = 0.1446, epoch_time = 363.4257sec\n",
      "total_loss = 11076068.8105, recall@50 = 0.1578, epoch_time = 365.1509sec\n",
      "total_loss = 10642570.7285, recall@50 = 0.1702, epoch_time = 362.8226sec\n",
      "total_loss = 10211023.2617, recall@50 = 0.1814, epoch_time = 363.1330sec\n",
      "total_loss = 9792816.9897, recall@50 = 0.1910, epoch_time = 372.7869sec\n",
      "total_loss = 9397754.8862, recall@50 = 0.1993, epoch_time = 374.1923sec\n",
      "total_loss = 9032962.3984, recall@50 = 0.2065, epoch_time = 369.2317sec\n",
      "total_loss = 8701422.8379, recall@50 = 0.2127, epoch_time = 370.2908sec\n",
      "total_loss = 8402663.6387, recall@50 = 0.2178, epoch_time = 371.0332sec\n",
      "total_loss = 8135004.2803, recall@50 = 0.2222, epoch_time = 373.4050sec\n",
      "total_loss = 7895518.6919, recall@50 = 0.2263, epoch_time = 365.7373sec\n",
      "total_loss = 7680583.3118, recall@50 = 0.2298, epoch_time = 375.1246sec\n",
      "total_loss = 7487136.3547, recall@50 = 0.2328, epoch_time = 368.0509sec\n",
      "total_loss = 7312310.6057, recall@50 = 0.2360, epoch_time = 368.3515sec\n",
      "total_loss = 7154523.2349, recall@50 = 0.2386, epoch_time = 328.9354sec\n",
      "total_loss = 7010682.5486, recall@50 = 0.2411, epoch_time = 326.8589sec\n",
      "total_loss = 6879304.1838, recall@50 = 0.2436, epoch_time = 326.7718sec\n",
      "total_loss = 6759129.1028, recall@50 = 0.2459, epoch_time = 329.4114sec\n",
      "total_loss = 6648329.1409, recall@50 = 0.2481, epoch_time = 327.6945sec\n",
      "total_loss = 6546407.6470, recall@50 = 0.2500, epoch_time = 326.7531sec\n",
      "total_loss = 6451967.9829, recall@50 = 0.2519, epoch_time = 326.9773sec\n",
      "total_loss = 6364108.7229, recall@50 = 0.2535, epoch_time = 327.4103sec\n",
      "total_loss = 6282303.7144, recall@50 = 0.2550, epoch_time = 324.6310sec\n",
      "total_loss = 6205764.0271, recall@50 = 0.2565, epoch_time = 324.0813sec\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "# epoch\n",
    "for epoch_id in range(1, num_epochs + 1):\n",
    "    train_loader = DataLoader(dataset, batch_size = 1024, shuffle = True)\n",
    "    start_epoch = timer()\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        user, item, neg_items = batch[0], batch[1], batch[2]\n",
    "        user, item, neg_items = user.cuda(), item.cuda(), neg_items.cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = model.distance_loss(user, item, neg_items) + model.cov_loss()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    model.eval()\n",
    "    recall_50 = model.evaluate(users, items, test_user, test_item)\n",
    "    print(\"total_loss = {:.4f}, recall@50 = {:.4f}, epoch_time = {:.4f}sec\".format(total_loss, recall_50, timer()-start_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mf_v1",
   "language": "python",
   "name": "mf_v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
