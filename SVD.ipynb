{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data.train\n",
    "test = data.test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatrixFactorization():\n",
    "    \n",
    "    \n",
    "    def __init__(self, train, test, k, learning_rate, reg_param, epochs, verbose = False):\n",
    "        \"\"\"\n",
    "        param R : Rating Matrix\n",
    "        param k : latent parameter\n",
    "        param learning_rate : alpha on weight update\n",
    "        param reg_param : regularization parameter\n",
    "        param epochs : training epochs\n",
    "        param verbose : print status\n",
    "        \"\"\"\n",
    "        \n",
    "        self._R = train\n",
    "        self._test = test\n",
    "        self._num_users, self._num_items = train.shape\n",
    "        self._k = k\n",
    "        self._learning_rate = learning_rate\n",
    "        self._reg_param = reg_param\n",
    "        self._epochs = epochs\n",
    "        self._verbose = verbose\n",
    "        \n",
    "    \n",
    "    def fit(self):\n",
    "        \"\"\"\n",
    "        training Matrix Factorization : update matrix latent weight and bias\n",
    "        \"\"\"\n",
    "        \n",
    "        # init latent features\n",
    "        self._P = np.random.normal(size=(self._num_users, self._k))\n",
    "        self._Q = np.random.normal(size=(self._num_items, self._k))\n",
    "        \n",
    "        # init biases\n",
    "        self._b_P = np.zeros(self._num_users)\n",
    "        self._b_Q = np.zeros(self._num_items)\n",
    "        self._b = np.mean(self._R[np.where(self._R != 0)]) # 0이 아닌 rating에 대해 평균\n",
    "        \n",
    "        # train while epochs\n",
    "        self._training_process = []\n",
    "        for epoch in range(self._epochs):\n",
    "            \n",
    "            # rating이 존재하는 index를 기준으로 training\n",
    "            for u in range(self._num_users):\n",
    "                for i in range(self._num_items):\n",
    "                    if self._R[u, i] > 0:\n",
    "                        self.gradient_descent(u, i, self._R[u, i])\n",
    "            \n",
    "            train_cost, test_cost = self.cost()\n",
    "            self._training_process.append((epoch, train_cost, test_cost))\n",
    "            \n",
    "            if self._verbose == True and ((epoch + 1) % 10 == 0 ):\n",
    "                print(\"Iteration : %d, train_cost = %.4f, test_cost = %.4f\" % (epoch+1, train_cost, test_cost))\n",
    "        \n",
    "    \n",
    "    def cost(self):\n",
    "        \"\"\"\n",
    "        compute RMSE\n",
    "        \"\"\"\n",
    "        xi, yi = self._R.nonzero() # 0 이 아닌 값의 index 반환\n",
    "        test_x, test_y = self._test.nonzero()\n",
    "        predicted = self.get_complete_matrix()\n",
    "        cost_train = 0\n",
    "        cost_test = 0\n",
    "        \n",
    "        for x, y in zip(xi, yi):\n",
    "            cost_train += pow(self._R[x, y] - predicted[x, y], 2)\n",
    "        \n",
    "        for i, j in zip(test_x, test_y):\n",
    "            cost_test += pow(self._test[i, j] - predicted[i, j], 2)\n",
    "        \n",
    "        return np.sqrt(cost_train/len(xi)), np.sqrt(cost_test/len(test_x))\n",
    "        \n",
    "    \n",
    "    def gradient(self, error, u, i):\n",
    "        \"\"\"\n",
    "        gradient of latent feature for GD\n",
    "        param error : rating - prediction error\n",
    "        param u : user index\n",
    "        param i : item index\n",
    "        \"\"\"\n",
    "        \n",
    "        dp = (error * self._Q[i, :]) - (self._reg_param * self._P[u, :])\n",
    "        dq = (error * self._P[u, :]) - (self._reg_param * self._Q[i, :])\n",
    "        \n",
    "        return dp, dq\n",
    "    \n",
    "    \n",
    "    def gradient_descent(self, u, i, rating):\n",
    "        \"\"\"\n",
    "        gradient descent function\n",
    "        param u : user index\n",
    "        param i : item index\n",
    "        param rating : rating of (u, i)\n",
    "        \"\"\"\n",
    "        \n",
    "        prediction = self.get_prediction(u,i)\n",
    "        error = rating - prediction\n",
    "        \n",
    "        self._b_P[u] += self._learning_rate * (error - self._reg_param * self._b_P[u])\n",
    "        self._b_Q[i] += self._learning_rate * (error - self._reg_param * self._b_Q[i])\n",
    "        \n",
    "        dp, dq = self.gradient(error, u, i)\n",
    "        self._P[u, :] += self._learning_rate * dp\n",
    "        self._Q[i, :] += self._learning_rate * dq\n",
    "        \n",
    "    \n",
    "    def get_prediction(self, u, i):\n",
    "        \"\"\"\n",
    "        get predicted rating by user i on item j\n",
    "        \"\"\"\n",
    "        return self._b + self._b_P[u] + self._b_Q[i] + self._P[u, :].dot(self._Q[i, :].T)\n",
    "\n",
    "    \n",
    "    def get_complete_matrix(self):\n",
    "        \"\"\"\n",
    "        computer complete matrix\n",
    "        \"\"\"\n",
    "        return self._b + self._b_P[:, np.newaxis] + self._b_Q[np.newaxis,:] + self._P.dot(self._Q.T)\n",
    "    \n",
    "    \n",
    "    def print_results(self):\n",
    "        \"\"\"\n",
    "        print fit results\n",
    "        \"\"\"\n",
    "\n",
    "        print(\"Final R matrix:\")\n",
    "        print(self.get_complete_matrix())\n",
    "        print(\"Final RMSE:\")\n",
    "        print(self._training_process[self._epochs-1][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration : 10, train_cost = 0.7035, test_cost = 1.6871\n",
      "Iteration : 20, train_cost = 0.6352, test_cost = 1.6354\n",
      "Iteration : 30, train_cost = 0.5849, test_cost = 1.6110\n",
      "Iteration : 40, train_cost = 0.5438, test_cost = 1.5922\n",
      "Iteration : 50, train_cost = 0.5102, test_cost = 1.5758\n",
      "Iteration : 60, train_cost = 0.4826, test_cost = 1.5611\n",
      "Iteration : 70, train_cost = 0.4598, test_cost = 1.5478\n",
      "Iteration : 80, train_cost = 0.4407, test_cost = 1.5358\n",
      "Iteration : 90, train_cost = 0.4246, test_cost = 1.5248\n",
      "Iteration : 100, train_cost = 0.4108, test_cost = 1.5146\n",
      "Iteration : 110, train_cost = 0.3989, test_cost = 1.5051\n",
      "Iteration : 120, train_cost = 0.3885, test_cost = 1.4961\n",
      "Iteration : 130, train_cost = 0.3793, test_cost = 1.4876\n",
      "Iteration : 140, train_cost = 0.3712, test_cost = 1.4795\n",
      "Iteration : 150, train_cost = 0.3639, test_cost = 1.4718\n",
      "Iteration : 160, train_cost = 0.3573, test_cost = 1.4645\n",
      "Iteration : 170, train_cost = 0.3513, test_cost = 1.4574\n",
      "Iteration : 180, train_cost = 0.3459, test_cost = 1.4506\n",
      "Iteration : 190, train_cost = 0.3409, test_cost = 1.4440\n",
      "Iteration : 200, train_cost = 0.3364, test_cost = 1.4377\n",
      "Iteration : 210, train_cost = 0.3322, test_cost = 1.4317\n",
      "Iteration : 220, train_cost = 0.3283, test_cost = 1.4258\n",
      "Iteration : 230, train_cost = 0.3246, test_cost = 1.4202\n",
      "Iteration : 240, train_cost = 0.3213, test_cost = 1.4148\n",
      "Iteration : 250, train_cost = 0.3181, test_cost = 1.4096\n",
      "Iteration : 260, train_cost = 0.3152, test_cost = 1.4046\n",
      "Iteration : 270, train_cost = 0.3124, test_cost = 1.3998\n",
      "Iteration : 280, train_cost = 0.3098, test_cost = 1.3951\n",
      "Iteration : 290, train_cost = 0.3074, test_cost = 1.3906\n",
      "Iteration : 300, train_cost = 0.3051, test_cost = 1.3862\n",
      "Final R matrix:\n",
      "[[4.54385201 3.62933279 4.2631546  ... 1.70109319 3.29693448 2.40615737]\n",
      " [3.59007305 3.18552394 1.65020577 ... 3.04379522 4.8524587  2.94906482]\n",
      " [3.47851068 4.4081379  0.04633335 ... 2.41052806 5.0442352  2.27920044]\n",
      " ...\n",
      " [4.93313293 2.96061698 2.90043992 ... 6.75925617 0.08509319 3.42861582]\n",
      " [4.57441173 2.63174744 3.50278158 ... 1.09199263 2.15394582 2.76768711]\n",
      " [4.67418613 5.15125779 3.49158505 ... 7.9820006  2.62062395 3.16157019]]\n",
      "Final RMSE:\n",
      "1.3862389204816992\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(7)\n",
    "    \n",
    "factorizer = MatrixFactorization(train, test, k=40, learning_rate=0.01, reg_param=0.01, epochs=300, verbose=True)\n",
    "factorizer.fit()\n",
    "factorizer.print_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RecSys_v1",
   "language": "python",
   "name": "recsys_v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
